### [关于如何准备一场演讲的清单](http://mp.weixin.qq.com/s/vExnnBm3rPp-8iapjt5cFw)

我们码农工作到一定年限后，职业发展高度其实是取决于沟通、影响力、统筹规划能力等很多软实力；演讲是提升个人影响力的很好方式，国内现在类似QCon，GMTC等大大小小的技术会议和沙龙有很多。那么怎么做好一场演讲呢，罗辑思维这篇文章有些点还是挺有借鉴意义的：

- 演讲切忌不做准备。以“今天我没有特别准备，随便说说，大家多包涵”开场，并不能表达谦虚，只会让听众感到不受尊重。
- 演讲最好是单层逻辑，切忌“三小点，下面还有五个分论点”。太多的层次，会加重听众的认知负担。
- 好的结尾，是全篇的点睛和复盘。结尾处务必进行一次总结：直接告诉听众，今天演讲的目的是什么。
- 所有的肢体语言都应该是有意识的，而不是慌张、无意识的乱动。有些习惯性动作，比如不自觉的挠头、摸鼻子、抖腿等，自己在平时很难注意到，但在台上就会被看得一清二楚，要在练习的时候去掉。
- 演讲的时候，眼神要环视和点视交替使用。环视，即环视全场；点视，即盯着某个人或者某个点看。如果现场观众人数较少，建议与每个听众都有5秒钟的眼神交流。好的演讲者，会让听众感觉就算在数百数千人的场景中，演讲者都是在面对面和自己讲话。
- 演讲超时是不礼貌的，会打乱整场活动的议程。控制演讲时长，是在演讲准备之初就要做好的功课。总共时长多少，哪块可以快速略过，哪块需要娓娓道来，提前做好时间分配。

### [腾讯移动分析Crash系统实时化建设与实践](http://ppt.geekbang.org/slide/show/874)

实时Crash分析系统是很重要的开发、运营基础设施，腾讯的这个分享非常赞，干货满满，非常值得反复研究。作者从 ***行业现状与挑战、腾讯移动分析解决方案、终端全平台建设、实时与运营系统建设*** 四个方面详细介绍了如何构建一个实时Crash分析系统。

现在移动App的质量情况：
- 用户遇到Crash的比例: 60%+
- 首次启动Crash，选择立即卸载的比例: 20%+
- 使用过程中，Crash后用户给应用打差评的比例: 70%+

构建实时Crash分析系统的挑战：
- 多平台覆盖: Android、iOS、游戏引擎
- 海量实时处理: 亿级日流量、实时还原、实时计算、实时告警
- 智能合并检索: 堆栈提取、智能合并、多维检索

看一下解决方案的总体架构

![](./Images/wk3/3.png)

需要解决的问题:
- 不同平台和CPU架构环境下异常数据、堆栈、环境属性、运行参数等数据的完整获取
- 将异常数据实时化处理，第一时间展现给用户，了解产品质量
- 准确的异常追踪过程，精确到行号的堆栈还原，是还原Crash现场的有力数据
- 完备的监控告警机制能及时监控App质量的波动，把握质量情况；云控助力远程解决问题

PPT中还介绍了 ***终端Crash采集平台建设的细节，包括Android C/C++ Crash处理流程、C/C++ Crash捕获流程、多维度数据采集辅助快速定位异常问题、实时还原系统建设(符号还原、行号还原、特征堆栈提取、实时多维度检索)、秒级实时计算系统、单价性能优化--海量并发连接、实时监控告警、云控系统*** ，更多细节请点击下方的“阅读原文”获取本文的带链接版本，然后通看完整的PPT。

一行代码，一个系统；腾讯推出的[腾讯移动分析](http://mta.qq.com/mta/)系统已近包含上面所有功能，开发者很容易就可以接入。

### [58同城Android客户端框架演进与实践](http://ppt.geekbang.org/slide/show/855)

作者分享了58同城Android客户端的架构演进过程，从最开始的纯Native到Hybrid，再到动态化、插件化、组件化。每一次改变都是源于实际的需求，比如从Native到Hybrid时为了解决开发效率问题，保证业务快速迭代，同时能快速修复线上问题、动态化是为了满足产品动态运营的需求、插件化和组件化是为了保证各个业务线并行开发以及各个业务单元的动态升级能力。通过这个PPT也可以快速的了解大型Android App的架构标配，更多细节可以看完整的PPT。

![](./Images/wk3/9.png)

### [The neural network zoo](http://www.asimovinstitute.org/neural-network-zoo/)

神经网络最近火的一塌糊涂，各种架构的神经网络层出不穷，同时也涌现了大量缩写名词：DCIGN，BiLSTM，DCGAN，让人感觉有点懵逼。因此作者总结盘点各种架构的神经网络，如下图：

![](./Images/wk3/4.png)

图中很清楚地展示了各种神经网络的架构以及他们的组成元素，但是这个架构图并不能展示每种神经网络的工作方式，也不一定涵盖了所有的类型的神经网络。以下是常见神经网络的简单介绍，完整的介绍请看原文。

前馈神经网络(FF)和感知器(P)

![](./Images/wk3/5.png)

前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层，直至输出层。其中第一层称为输入层。最后一层为输出层．中间为隐含层，简称隐层。隐层可以是一层。也可以是多层。整个网络中无反馈，可用一个有向无环图表示。

反向传播算法可用来学习这个前馈神经网络的权值，采用梯度下降方法试图最小化网络输出值和目标值之间的误差平方。

具体细节可以参考 Rosenblatt, Frank的论文[ The perceptron: a probabilistic model for information storage and organization in the brain ](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)

卷积神经网络（CNN：convolutional neural networks）

![](./Images/wk3/7.png)

卷积神经网络是一种前馈神经网络，对于大型图像处理有出色表现。20世纪60年代，Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性，继而提出了卷积神经网络。卷积神经网络由一个或多个卷积层和顶端的全连通层组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构，主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。

具体细节可以参考LeCun, Yann, et al的论文[ Gradient-based learning applied to document recognition ](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)

递归神经网络（RNN：Recurrent neural networks）

![](./Images/wk3/8.png)

递归神经网络（RNN）是两种人工神经网络的总称。一种是时间递归神经网络（recurrent neural network），另一种是结构递归神经网络（recursive neural network）。时间递归神经网络的神经元间连接构成有向图，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。RNN一般指代时间递归神经网络。单纯递归神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题

具体细节可以参考Elman, Jeffrey L.的论文[ Finding structure in time ](https://crl.ucsd.edu/~elman/Papers/fsit.pdf)

### 参考资料
- http://www.asimovinstitute.org/neural-network-zoo/
- http://www.asimovinstitute.org/neural-network-zoo-prequel-cells-layers/
- https://github.com/Tencent/mars
- http://data.qq.com/
- http://mta.qq.com/mta/
- http://mp.weixin.qq.com/s/305V7bDTJb8jOInz9l_xTA