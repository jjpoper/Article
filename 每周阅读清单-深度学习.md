### [一个神经网络学习一切！谷歌又放了个大卫星](http://mp.weixin.qq.com/s/ARnDNJkLSkTOBtQ7w50cSg)

深度学习挺好，但是每解决一个问题都得建一个模型，还要花好长时间调参、训练，太麻烦了。于是乎Google Brain的同学搞出了一个在各种领域的很多问题上效果都不错的模型MultiModel，并且号称："One Model To Learn All"。

目前相关工作人员正在用下面这8个数据集训练这个模型，期望能识别ImageNet图像、进行多种语言的翻译、在COCO数据集上根据图片生成文字说明、做语音识别、做英文语法分析：

- 《华尔街日报》语音语料库
- ImageNet数据集
- COCO图片说明数据集
- 《华尔街日报》句法分析数据集
- WMT英-德翻译预料库
- WMT德-英翻译预料库
- WMT英-法翻译预料库
- WMT法-英翻译预料库

为了在不同维度、大小、类型的数据上进行训练，MultiModel由多个特定模式的子网络对这些输入数据进行转换，并放进一个共同的表示空间。这些子网络叫做“模式网络(modelity nets)”。如图所示，MultiModel由几个模式网络、一个编码器、一个I/O混合器、一个自回归解码器构成，模型的主题包含了多个卷积层、注意力机制和稀疏门控层等。具体的细节可以在看[One Model To Learn All](https://arxiv.org/pdf/1706.05137.pdf)这篇论文。

![](./Images/wk2/1.png)

